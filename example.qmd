---
title: "example introduction start"
format: html
editor: visual
---

## Computation evaluation of imputation methodology
Missing data continue to pose a challenge to researchers across most scientific domains. In recent years, imputation methodology has grown increasinly mature ane sophisticated solutions to complex problems have been proposed [evt REF]. Most of these solutions rely on algorithms that take incomplete data as input and provide completed data as output. These solutions use some form automated computation routine to generate that output. In practice, many analysts now rely on such computational routines to solve the missing-data problem long before they consider the deeper question of how such routines behave and why this specific output is generated.

Contemporary imputation methods are often algorithmic, iterative, highly automated and aimed to produce results with remarkable speed. However, the increasing reliance on computer generated data-driven computational output introduces a new, subtle problem: are the solutions presented to us by these algorithms trustworthy, valid and should we even dare to rely on them? After all, missing data is often considere a mere throughput step and the grand aim of the study is to analyze the data as if all values were completely observed. If we are satisfied with our imputations given the scope of the study - how many of us would even consider to go one step back and consider the relative validity of the generated result in practice?

This paradox is not unique to missing data. Consider the field of meteorology. Modern weather forecasting systems rely on intricate numerical models, satellite feeds, and machine-learned correction layers. These systems ingest terabytes of atmospheric information and output forecasts that are trusted by meteorologists, pilots, farmers, governments, and people with a strong preference for rain-free commutes. Yet, the reliability of these predictions depends crucially on the unseen computational machinery that transforms raw observations into actionable forecasts. When a storm is predicted to grow or to change course, the result is often accepted because the model is considered to be sufficiently sophisticated. We rarely accept the model because of its internal validity or because its convergence diagnostics were scrutinized [is dit wel zo?]. We only return to the model when the forecast fails spectacularly, suddenly demanding explanations that should perhaps have been examined beforehand.

The same tendency arises nowadays in statistical modeling, data science and AI (which IMO stands for automated information that humans tend to percieve as intelligence). As the complexity of imputation algorithms has increased, so too has the tendency to treat their output as definitive. Many analysts accept imputed values, completed datasets, or model estimates without a careful evaluation of the algorithmic steps that produced them. This is particularly tricky because most modern imputation techniques, such as - but not limited to - fully conditional specification, chained equations, Bayesian updating, and predictive mean matching—depend critically on assumptions about data structure, convergence, randomness, congeniality and compatibility. Their performance may vary dramatically across missingness mechanisms, data-generating processes, and computational settings. Without explicit assessment, the plausibility and validity of the results remain uncertain. After all, an imputed value that is numerically convenient is not necessarily scientifically defensible or theoretically explainable.

Drawing inference from incomplete data is therefore not merely a question of choosing an imputation method, but also of evaluating its computational properties. A modern imputer faces a two-fold task: first, to obtain imputations, and second, to verify that those imputations genuinely respect the structure, restrictions, and uncertainty inherent in the data. This second task—algorithmic evaluation—is conceptually straightforward but often neglected in practice. Analysts may examine means and variances of the completed data while ignoring possible non-convergence, model misspecification, implausible draws, or inconsistencies between observed and imputed distributions. Yet the validity of any inference depends as much on these algorithmic subtleties as on the chosen statistical model.

Evaluating imputation methodology is therefore both an art and a science. On the one hand, it requires simulation studies that accurately reflect real-world data structures, missingness mechanisms, and inferential goals. On the other hand, it requires diagnostics capable of detecting computational pathologies: non-convergence, instability, implausibility, inefficiency, and violations of underlying model assumptions. Neither part is trivial. Poorly designed simulations may offer false reassurance, while inadequate diagnostics may mask critical defects in the imputation algorithm. As a result, methodological advancements in imputation or real world applications that rely thereon, would risk being adopted without a clear understanding of when they work, how well they work, and under what conditions they fail.

This dissertation addresses the computational evaluation of imputation methodology. Its central motivation is the growing recognition amongst critical statisticians that generated imputation output cannot be accepted at face value; it must be rigorously examined. The focus is on designing principled computational evaluations (CHAPTER 1 - ggmice?), developing diagnostics that reveal algorithmic behavior (CHAPTER 2 missing the point?), and exploring how simulation design affects the interpretation of imputation performance (Chapter 3 on the evaluation of...?). The overarching aim is to provide a framework that helps researchers move beyond the black-box use of imputation routines and toward a deeper, more systematic assessment of their statistical and computational validity.

